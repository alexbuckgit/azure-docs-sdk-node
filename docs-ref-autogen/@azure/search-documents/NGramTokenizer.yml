### YamlMime:TSTypeAlias
name: NGramTokenizer
uid: '@azure/search-documents.NGramTokenizer'
package: '@azure/search-documents'
summary: >-
  Tokenizes the input into n-grams of the given size(s). This tokenizer is
  implemented using Apache Lucene.
fullName: NGramTokenizer
remarks: ''
isPreview: false
isDeprecated: false
syntax: |
  type NGramTokenizer = BaseLexicalTokenizer & {
    maxGram?: number,
    minGram?: number,
    odatatype: "#Microsoft.Azure.Search.NGramTokenizer",
    tokenChars?: TokenCharacterKind[],
  }
