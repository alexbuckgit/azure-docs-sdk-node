### YamlMime:TSTypeAlias
name: EdgeNGramTokenizer
uid: '@azure/search-documents.EdgeNGramTokenizer'
package: '@azure/search-documents'
summary: >-
  Tokenizes the input from an edge into n-grams of the given size(s). This
  tokenizer is implemented using Apache Lucene.
fullName: EdgeNGramTokenizer
remarks: ''
isPreview: false
isDeprecated: false
syntax: |
  type EdgeNGramTokenizer = BaseLexicalTokenizer & {
    maxGram?: number,
    minGram?: number,
    odatatype: "#Microsoft.Azure.Search.EdgeNGramTokenizer",
    tokenChars?: TokenCharacterKind[],
  }
